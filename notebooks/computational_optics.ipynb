{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Optics\n",
    "\n",
    "Conventional imaging systems employ compound refractive lens systems that are typically hand-engineered for image quality in isolation of the downstream camera task. Departing from this design paradigm, a growing body of work in computational photography [Haim et al. 2018; Horstmeyer et al. 2017] has explored the design of specialized lens system with diffractive optical elements (DOEs). \n",
    "\n",
    "\n",
    "As a specific example, we consider end-to-end computational optics that jointly optimize a diffractive optical element (DOE) and an image reconstruction algorithm,  where the observation $y$ is obtained by convolving a clear image $x$ by the point spread function (PSF) of DOE as,\n",
    "$$\n",
    "    y =  D\\left(x;\\, \\theta_{DOE} \\right) + \\epsilon, \n",
    "$$\n",
    "where  $D(\\cdot; \\theta_{DOE})$ indicates a shift-invariant convolution process with an optical kernel, i.e., PSF, derived from a DOE image formation model parameterized by $\\theta_{DOE}$, and $\\epsilon$ is measurement noise, e.g., Poissionian-Gaussian noise. \n",
    "To reconstruct target image $x$ from noise-contaminated measurements $y$, we minimize the sum of a data-fidelity $f$ and regularizer term $r$ as\n",
    "\\begin{align}\n",
    "    \\mathop{\\mathrm{min}}_{x \\in \\mathbb{R}^n} ~ f \\left( D\\left(x;\\, \\theta_{DOE} \\right) \\right) + r \\left(x ; \\, \\theta_r \\right).\n",
    "\\end{align}\n",
    "\n",
    "In this tutorial, we will show three different approaches for solving this problem, including\n",
    "\n",
    "1. Base DOE model with plug-and-play proximal algorithm in âˆ‡-Prox.\n",
    "2. Conventional joint optimization of DOE model and black-box deep deconvolution neural network. [Metzler et al. 2020]\n",
    "3. Joint optimization of DOE model and differentiable proximal solver in âˆ‡-Prox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dprox import *\n",
    "from dprox.utils import *\n",
    "from dprox.utils.examples.optic import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOE with baseline profile\n",
    "\n",
    "Let's first build a doe model. Notice you could freely adjust the physical optical setup parameters using `DOEModelConfig` class. But please assure you have some domain knowledge of [**Fourier Optics**](https://en.wikipedia.org/wiki/Fourier_optics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DOEModelConfig()\n",
    "rgb_collim_model = build_doe_model(config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By cropping the center region of the point spread function (PSF) of the baseline doe model, we could see visualize something like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fresnel_phase_c = build_baseline_profile(rgb_collim_model)\n",
    "psf = rgb_collim_model.get_psf(fresnel_phase_c)\n",
    "\n",
    "psf_baseline = crop_center_region(normalize_psf(to_ndarray(psf, debatch=True), range=0.001, mode='all'))\n",
    "imshow(psf_baseline, titles=[\"PSF of a Green-focused Fresnel Lens\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's load a sample image, and generate the blurred observation with the baseline DOE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = load_sample_img(get_path('sample_data/optic_sample.png'))\n",
    "\n",
    "sigma = 7.65 / 255\n",
    "noise = torch.randn(*gt.shape, device=gt.device) * sigma\n",
    "psf = rgb_collim_model.get_psf(fresnel_phase_c)\n",
    "inp_baseline = img_psf_conv(gt, psf, circular=True) + noise\n",
    "imshow(gt, inp_baseline, titles=['Original scene', 'Image from a green-focused Fresnel lens'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reconstruct the clean image from the blurred observation, we could use plug-and-play proximal algorithm.\n",
    "\n",
    "Here, we show an example of how to solve it with ADMM and a deep image prior in âˆ‡-Prox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable()\n",
    "y = Placeholder()\n",
    "PSF = Placeholder()\n",
    "data_term = sum_squares(conv_doe(x, PSF, circular=True), y)\n",
    "reg_term = deep_prior(x, denoiser='ffdnet_color')\n",
    "solver = compile(data_term + reg_term, method='admm')  # compile the optimization problem into an ADMM solver\n",
    "\n",
    "inp = inp_baseline.float()\n",
    "y.value, PSF.value = inp, psf\n",
    "max_iter = 10\n",
    "rhos, sigmas = log_descent(49, 7.65, max_iter, sigma=max(0.255 / 255, sigma))  # set inner loop parameters using a log descent policy\n",
    "out_baseline = solver.solve(x0=inp, rhos=rhos, lams={reg_term: sigmas}, max_iter=max_iter)\n",
    "\n",
    "imshow(out_baseline, out_baseline[:,:,460:660,140:340])\n",
    "print(psnr(out_baseline, gt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we could see that naive doe model does not work well with existing post-processing techniques, such as the proximal algorithm we used here. To further increase cooperations between imaging model and processing algorithms. We could jointly optimize them. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventional Joint Optimization of DOE Model and Deep Deconvolution Network\n",
    "\n",
    "In DeepOptics-UNet, Metzler et al propose to jointly optimize the DOE model and a post-deconvolution network. This results in a learned DOE model whose PSF can be visualized as follow. We could see the learned PSF is quite different from previous one, which focus more on blue channel instead of green channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dprox.utils.examples.optic.unet import U_Net\n",
    "solver = U_Net(3,3).to(device)\n",
    "rgb_collim_model = build_doe_model()\n",
    "\n",
    "ckpt = torch.load('checkpoints/joint_unet.pth')\n",
    "rgb_collim_model.load_state_dict(ckpt['model'])\n",
    "solver.load_state_dict(ckpt['solver'])    \n",
    "\n",
    "psf = rgb_collim_model.get_psf()\n",
    "psf_unet = crop_center_region(normalize_psf(to_ndarray(psf, debatch=True), mode='band'))\n",
    "imshow(psf_unet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the performance of joint optimization, we could capture/generate an observation and see what results would the deconvolution network reconstruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = gt.to(device).float()\n",
    "psf = rgb_collim_model.get_psf()\n",
    "inp = img_psf_conv(gt, psf, circular=True)\n",
    "inp_unet = inp + torch.randn(*inp.shape, device=inp.device) * sigma\n",
    "\n",
    "out_unet = solver(inp_unet)\n",
    "imshow(inp_unet, out_unet, out_unet[:,:,460:660,140:340])\n",
    "print(psnr(out_unet, gt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘» Oops! We could see that the results is much better, which demonstrate the effectiveness of joint optimization. \n",
    "\n",
    "In fact, the above approach jointly optimize the DOE model and a black-box deconvolution network. However, how about the joint optimization of the DOE model with a formal optimization deconvolution algorithm, e.g. proximal algorithm ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOE with Learned Profile in âˆ‡-Prox\n",
    "\n",
    "âˆ‡-Prox builds differentiable proximal solver that can be used to jointly optimize every input to the solver, e.g., the doe model. \n",
    "\n",
    "For simplicity, we here provide the learned doe model by âˆ‡-Prox. The training code for it can be found in [computational_optics](). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load learned doe model\n",
    "rgb_collim_model.rhos = nn.parameter.Parameter(torch.zeros((max_iter)))\n",
    "rgb_collim_model.sigmas = nn.parameter.Parameter(torch.zeros((max_iter)))\n",
    "rgb_collim_model.load_state_dict(torch.load('checkpoints/joint_proximal.pth')['model'])\n",
    "\n",
    "# visualize point spread function\n",
    "psf = rgb_collim_model.get_psf()\n",
    "psf_dprox = crop_center_region(normalize_psf(to_ndarray(psf, debatch=True), range=0.05, mode='band'))\n",
    "imshow(psf_dprox)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps interestingly, the above visualization highlights that the optimized PSF resulting from the end-to-end optimization problem from above drastically differs from the one optimized with a UNet post-processor. \n",
    "\n",
    "The phase pattern we learn focuses the three wavelength bands as highly chromatic PSFs for each channel, that is, the red green and blue PSFs only focus for the specific channel while spreading out energy for other wavelengths over the entire sensor. \n",
    "\n",
    "By spatially separating the corresponding phase patterns, our co-designed network is able to find these chromatic PSFs. As such,  âˆ‡-Prox allowed us to find a novel point in the design space -- turning an out-of-focus deconvolution problem into a transverse chromatic alignment problem. ðŸ™€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate input\n",
    "gt = gt.to(device).float()\n",
    "psf = rgb_collim_model.get_psf()\n",
    "inp = img_psf_conv(gt, psf, circular=True)\n",
    "inp_dprox = inp + torch.randn(*inp.shape, device=inp.device) * sigma\n",
    "\n",
    "# build solver\n",
    "x = Variable()\n",
    "y = Placeholder()\n",
    "PSF = Placeholder()\n",
    "data_term = sum_squares(conv_doe(x, PSF, circular=True), y)\n",
    "reg_term = deep_prior(x, denoiser='ffdnet_color')\n",
    "solver = compile(data_term + reg_term, method='admm')\n",
    "\n",
    "# solve the problem\n",
    "y.value = inp_dprox\n",
    "PSF.value = psf\n",
    "out_dprox = solver.solve(x0=inp_dprox,\n",
    "                    rhos=rgb_collim_model.rhos,\n",
    "                    lams={reg_term: rgb_collim_model.sigmas},\n",
    "                    max_iter=max_iter)\n",
    "\n",
    "imshow(inp, out_dprox, out_dprox[:,:,460:460+200,140:140+200])\n",
    "print(psnr(out_dprox, gt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, the model-based proximal optimization solver compiled with âˆ‡-Prox finds a better local minimum with a significantly improved end-to-end loss, which validates the effectiveness of the differentiable pipelines we compile."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrap up the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(psf_baseline, psf_unet, psf_dprox)\n",
    "imshow(inp_baseline, inp_unet, inp_dprox)\n",
    "imshow(out_baseline, out_unet, out_dprox)\n",
    "imshow(out_baseline[:,:,460:460+200,140:140+200], out_unet[:,:,460:460+200,140:140+200], out_dprox[:,:,460:460+200,140:140+200])\n",
    "print('PSNR - Baseline: {} DeepOptics-Unet: {} Ours: {}'.format(psnr(gt, out_baseline), psnr(gt, out_unet), psnr(gt, out_dprox)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[Haim et al. 2018] Harel Haim, Shay Elmalem, Raja Giryes, Alex Bronstein, and Emanuel Marom. 2018. Depth Estimation From a Single Image Using Deep Learned Phase Coded Mask. IEEE Transactions on Computational Imaging 4 (2018), 298â€“310.\n",
    " \n",
    "[Horstmeyer et al. 2017] Roarke Horstmeyer, Richard Y. Chen, Barbara Kappes, and Benjamin Judkewitz. 2017. Convolutional neural networks that teach microscopes how to image. ArXiv abs/1709.07223 (2017).\n",
    "\n",
    "[Metzler et al. 2020] Christopher A Metzler, Hayato Ikoma, Yifan Peng, and Gordon Wetzstein. 2020. Deep optics for single-shot high-dynamic-range imaging. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1375â€“1385."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a870b966584756aeba6c824cc21ec4ca49c058caa364cbaa3ff3aca61090be1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
