{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix-free Differentiable Linear Solver\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Zeqiang-Lai/Delta-Prox/blob/master/notebooks/csmri.ipynb) \n",
    "\n",
    "\n",
    "In this tutorial, we provide a step by step derivation of the matrix-free differentiable linear solver mentioned in ∇-Prox.\n",
    "\n",
    "Recall that our goal is to find the gradient of the output of a linear solver $\\bar{x}$\n",
    "$$\n",
    "\\bar{x} = \\text{Solve}(Kx=b)\n",
    "$$\n",
    "with respect to the parameters in the solved linear system, such as $\\frac{\\partial \\bar{x}}{\\partial K}$ and $\\frac{\\partial \\bar{x}}{\\partial b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to install dprox if your are in online google colab notebook\n",
    "# !pip install dprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Approach with Auto-Diff\n",
    "\n",
    "Let us first derive the gradient with auto-differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "theta = torch.randn((32,32), requires_grad=True)  # define parameter of the linOp K\n",
    "K = theta * 2\n",
    "x = torch.randn((32))\n",
    "b = K @ x\n",
    "b = b.clone().detach().requires_grad_(True)\n",
    "\n",
    "xhat = torch.linalg.solve(K, b)\n",
    "\n",
    "loss = xhat.mean()\n",
    "loss.backward()\n",
    "\n",
    "print(theta.grad.shape)\n",
    "print(b.grad.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Differentiation\n",
    "\n",
    "Auto-diff can be used to efficiently differentiate fast direct linear solvers but is often intractable for iterative linear solvers. \n",
    "\n",
    "In ∇-Prox, we provide an optimized routine to compute the analytic derivatives of linear (iterative) solver outputs with respect to the parameters of linear operators $\\theta$ and $b$. \n",
    "\n",
    "###  Derivation of $\\frac{\\partial \\bar{x}}{\\partial b}$\n",
    "\n",
    "Specifically, we differentiate both sides of $K\\bar{x} =b$ to obtain the derivatives $\\frac{\\partial \\bar{x}}{\\partial b}$ and $\\frac{\\partial \\bar{x}}{\\partial \\theta}$ as\n",
    "\n",
    "$$\n",
    "\\partial K \\bar{x} + K \\partial \\bar{x} = \\partial b \\\\\n",
    "\\partial \\bar{x} = K^{-1} (-\\partial K \\bar{x} + \\partial b)\n",
    "$$\n",
    "\n",
    "from which the gradient $\\frac{\\partial \\bar{x}}{\\partial b} = K^{-1}$ can be easily derived. Typically, we are more interested in the gradient of $b$ with respect to a scalar loss function $\\mathcal{L}$, which can be obtained with the chain rule of differential calculus.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} =  \\left (\\frac{\\partial \\bar{x}}{\\partial b} \\right )^T \\frac{\\partial \\mathcal{L}}{\\partial \\bar{x}} =  K^{-T} \\frac{\\partial \\mathcal{L}}{\\partial \\bar{x}}\n",
    "$$\n",
    "\n",
    "Since all the linear operators in our system are matrix-free, we cannot directly evaluate the above formula for gradient computing. Instead, we transform it into\n",
    "$$\n",
    " K^T \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial \\mathcal{L}}{\\partial x}  \n",
    "$$\n",
    "where the right-hand-side is the Jacobian of $\\mathcal{L}$ with respect to $x$ that can be efficiently evaluated with auto-diff systems. The calculation of gradient $\\frac{\\partial \\mathcal{L}}{\\partial b}$ has thus been converted into solving a linear system, requiring significantly less memory. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above derivation assumes the gradient layout of \n",
    "> \n",
    "> dx/db = [dx1/db1, dx1/db2, ..., dxn/dbn; dx2/db1, ...; dx3/db1, ...]\n",
    "> \n",
    "> Note that the gradient layout of torch.autograd.functional.jacobian is the same as above.\n",
    ">\n",
    "> See also: https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1406, -0.0938,  0.2671, -0.1739, -0.1323])\n",
      "tensor(1.4901e-08, grad_fn=<MaxBackward1>)\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "theta = torch.randn((5,5), requires_grad=True)\n",
    "K = theta * 2\n",
    "x = torch.randn(5)\n",
    "b = K @ x\n",
    "b = b.clone().detach().requires_grad_(True)\n",
    "\n",
    "xhat = torch.linalg.solve(K, b)\n",
    "xhat.retain_grad()  # retain non-leaf gradient for analytical compute\n",
    "\n",
    "loss = xhat.mean()\n",
    "loss.backward()\n",
    "\n",
    "# analytical gradient using implicit differentiation \n",
    "db = torch.inverse(K.T) @ xhat.grad  \n",
    "db2 = torch.linalg.solve(K.T, xhat.grad)\n",
    "\n",
    "# analytical gradient versus auto-grad \n",
    "print(b.grad)\n",
    "print((b.grad - db).abs().max())  \n",
    "print(torch.allclose(b.grad, db, rtol=1e-6))\n",
    "print(torch.allclose(b.grad, db2, rtol=1e-6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Derivation of $\\frac{\\partial \\bar{x}}{\\partial \\theta}$\n",
    "\n",
    "Similarly, the gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$ with respect to the parameters $\\theta$ of the linear operator $K$ can be derived as\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\left (\\frac{\\partial \\bar{x}}{\\partial \\theta} \\right )^T  \\frac{\\partial \\mathcal{L}}{\\partial \\bar{x}}  \\quad \\text{s.t.} \\quad \\frac{\\partial \\bar{x}}{\\partial \\theta} = -K^{-1}  \\frac{\\partial K}{\\partial \\theta} \\bar{x} \\,.\n",
    "$$\n",
    "\n",
    "Again, $\\frac{\\partial K}{\\partial \\theta}$ cannot be evaluated directly as we consider matrix-free linear operators. To circumvent this obstacle, we use the fact that \n",
    "$$\\frac{\\partial K}{\\partial \\theta}\\bar{x}=\\frac{\\partial b}{\\partial \\theta}$$ \n",
    "to transform it into\n",
    "$$\n",
    "K \\frac{\\partial \\bar{x}}{\\partial \\theta} = -  \\frac{\\partial b}{\\partial \\theta} ,\n",
    "$$\n",
    "where $\\frac{\\partial b}{\\partial \\theta}$ can be computed by backpropagating the forward computation $K\\bar{x}=b$. As such, the calculation of gradients $\\frac{\\partial \\mathcal{L}}{\\partial b}$ and $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$ is converted into solving linear systems during backpropagation without requiring storing intermediate states, thereby significantly reducing memory consumption and saving computation time. \n",
    "\n",
    "> Note that we assume $\\theta$ to have the same shape as $K$, so that the shape of $\\frac{\\partial K}{\\partial \\theta}\\bar{x}=\\frac{\\partial b}{\\partial \\theta}$ holds. The gradient with the real $\\bar{\\theta}$ can be automatically tracked by auto-diff if we know the function that transforms $\\theta$ into $\\bar{\\theta}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference Implementation with the Explicit Matrix**\n",
    "\n",
    "Suppose $K \\in \\mathrm{R}^{R\\times C}$, $\\theta \\in \\mathrm{R}^{R2\\times C2} $, $x \\in \\mathrm{R}^{N}$, $b \\in \\mathrm{R}^{N}$.\n",
    "\n",
    "Since $K$ is a square matrix, $R$, $C$, $R2$, $C2$, $N$ are of the same value. We simply use different symbols to better illustrate the gradient layout.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\bar{x}}{\\partial \\theta} = -K^{-1}  \\frac{\\partial K}{\\partial \\theta} \\bar{x}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\left (\\frac{\\partial \\bar{x}}{\\partial \\theta} \\right )^T  \\frac{\\partial \\mathcal{L}}{\\partial \\bar{x}}\n",
    "$$\n",
    "\n",
    "Note that this might be confused for the shape computation. However, keep in mind, that we are interested in the gradient layout. As all $R$, $C$, $R2$, $C2$, $N$ are of the same value, they are valid for matrix multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3991e-08, grad_fn=<MeanBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# define a linOp depending on the parameter theta\n",
    "Kmat = lambda theta: theta * 2\n",
    "\n",
    "dK_dtheta = jacobian(Kmat, theta) # [R x C] x [R2 x C2]\n",
    "\n",
    "# In PyTorch, dK_dtheta @ xhat is recognized as batched matrix multiplication\n",
    "# It would be [R x C] x [R2 x C2] @ [N x 1]\n",
    "# so dK_dtheta @ xhat actually returns [R x C] x R2\n",
    "\n",
    "# Method 1\n",
    "# R x C @ [R x C] x R2 = R x C x R2\n",
    "dxhat_dtheta = - K.inverse() @ dK_dtheta @ xhat\n",
    "\n",
    "# Method 2\n",
    "# In theory, dxhat_dtheta should be N x [R2 x C2], \n",
    "# but torch.linalg.solve returns [R2 x C2] x N, \n",
    "# Note: K = [R x C], -(dK_dtheta @ xhat) = [R2 x C2](batch size) x N x 1\n",
    "dxhat_dtheta = torch.linalg.solve(K, -(dK_dtheta @ xhat).unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# Therefore, we do not need to transpose dxhat_dtheta here.\n",
    "dloss_dtheta = dxhat_dtheta @ xhat.grad\n",
    "\n",
    "print(torch.mean(torch.abs(dloss_dtheta - theta.grad)))\n",
    "print(torch.allclose(dloss_dtheta, theta.grad, rtol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3677e-08, grad_fn=<MeanBackward0>)\n",
      "tensor([[-1.0499e-01,  1.0606e-02,  3.8049e-02,  9.7144e-05, -9.9203e-01],\n",
      "        [ 3.4649e-02, -3.5001e-03, -1.2557e-02, -3.2060e-05,  3.2739e-01],\n",
      "        [-1.1986e-01,  1.2108e-02,  4.3439e-02,  1.1091e-04, -1.1326e+00],\n",
      "        [ 4.9314e-02, -4.9815e-03, -1.7872e-02, -4.5629e-05,  4.6596e-01],\n",
      "        [-2.3773e-01,  2.4014e-02,  8.6152e-02,  2.1996e-04, -2.2462e+00]])\n",
      "tensor([[-1.0499e-01,  1.0606e-02,  3.8049e-02,  9.7131e-05, -9.9203e-01],\n",
      "        [ 3.4649e-02, -3.5002e-03, -1.2557e-02, -3.2051e-05,  3.2739e-01],\n",
      "        [-1.1986e-01,  1.2108e-02,  4.3439e-02,  1.1087e-04, -1.1326e+00],\n",
      "        [ 4.9314e-02, -4.9815e-03, -1.7871e-02, -4.5622e-05,  4.6596e-01],\n",
      "        [-2.3773e-01,  2.4014e-02,  8.6152e-02,  2.1991e-04, -2.2462e+00]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "theta = torch.randn((5,5), requires_grad=True)\n",
    "x = torch.randn(5)\n",
    "\n",
    "def f(theta):\n",
    "    K = theta * 2\n",
    "    b = K @ x\n",
    "    b = b.clone().detach().requires_grad_(True)\n",
    "    xhat = torch.linalg.solve(K, b)\n",
    "    return xhat\n",
    "\n",
    "# Directly evaluate df_dtheta using auto-grad (note that this naive approach scales very poorly)\n",
    "jab = jacobian(f, theta)  \n",
    "xhat = xhat.clone().detach().requires_grad_()\n",
    "# xhat.retain_grad()\n",
    "\n",
    "loss = xhat.mean()\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "dtheta = jab.permute(1,2,0) @ xhat.grad\n",
    "print(torch.mean(torch.abs(dtheta - dloss_dtheta)))\n",
    "print(jab.permute(1,2,0)[0])\n",
    "print(dxhat_dtheta[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix-Free Reference Implementation**\n",
    "\n",
    "Suppose $K \\in \\mathrm{R}^{R\\times C}$, and $\\theta \\in \\mathrm{R}^{R2 \\times C2} $, and $x \\in \\mathrm{R}^{N}$, $b \\in \\mathrm{R}^{N}$.\n",
    "\n",
    "Since $K$ is a square matrix, $R$, $C$, $R2$, $C2$, $N$ are of the same value. We simply use different symbols to better illustrate the gradient layout.\n",
    "\n",
    "$$\n",
    "K \\frac{\\partial \\bar{x}}{\\partial \\theta} = -  \\frac{\\partial b}{\\partial \\theta} ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\left(\\frac{\\partial \\bar{x}}{\\partial \\theta} \\right )^T  \\frac{\\partial \\mathcal{L}}{\\partial \\bar{x}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "tensor(2.6605e-08, grad_fn=<MeanBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "theta = torch.randn((5,5), requires_grad=True)\n",
    "K = theta * 2\n",
    "x = torch.randn(5)\n",
    "b = K @ x\n",
    "b = b.clone().detach().requires_grad_(True)\n",
    "\n",
    "xhat = torch.linalg.solve(K, b)\n",
    "xhat.retain_grad()\n",
    "\n",
    "loss = xhat.mean()\n",
    "loss.backward()\n",
    "\n",
    "def linop(theta):\n",
    "    return (theta*2) @ xhat\n",
    "\n",
    "# R2 x C2 here works like batch size\n",
    "db_dtheta = jacobian(linop, theta).permute(1,2,0).unsqueeze(-1) # [R2 x C2] x N x 1\n",
    "\n",
    "# in theory, dxhat_dtheta should be N x [R2 x C2], \n",
    "# but torch.linalg.solve return [R2 x C2] x N, \n",
    "# Note: K = [R x C], db_dtheta = [R2 x C2](batch size) x N x 1\n",
    "dxhat_dtheta = torch.linalg.solve(K, -db_dtheta).squeeze(-1)\n",
    "\n",
    "# therefore, we don't need to transpose dxhat_dtheta here.\n",
    "dLoss_dtheta = dxhat_dtheta @ xhat.grad\n",
    "\n",
    "print(dLoss_dtheta.shape)\n",
    "print(torch.mean(torch.abs(dLoss_dtheta - theta.grad) / torch.max(torch.abs(theta.grad))))\n",
    "print(torch.allclose(dLoss_dtheta, theta.grad, rtol=1e-6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
